{"changed":true,"filter":false,"title":"preprocess_data.py","tooltip":"/aws_services_py/preprocess_data.py","value":"\nimport sys\n# Surpress warnings:\ndef warn(*args, **kwargs):\n    pass\nimport warnings\nwarnings.warn = warn\n\n#%pylab inline\n#get_ipython().run_line_magic('matplotlib', 'inline')\n#get_ipython().run_line_magic('config', \"InlineBackend.figure_formats = ['retina']\")\nimport uuid\nimport time\nimport json\nimport boto3\nimport pandas as pd\nfrom io import BytesIO\nimport seaborn as sns\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import Lasso\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\nimport boto3_rds_pandas as mysql\nimport joblib\n#setup logging for algorithm\nalgo_type = 'Decision Tree'\nalgo_id =  str(int(time.time())) + '_' + algo_type\nalgo_name =  'Decision Tree user_risk'\n\nprint('algo_name')\nprint(algo_name)\n\ny_col = \"resulted_in_reconcilation\" ## known values of whether or not a transaction resulted in a reconciliation\nshow_logs = False\n\ns = StandardScaler()\nmms = MinMaxScaler()\nsns.set()\n\n\nquery = \"SELECT transaction_id, months, transaction_type,  trust_status, complete_sales_a, reconciliations_c,user_has_ssn,resulted_in_reconcilation FROM stats.user_scores_both WHERE 1=1  \"\ndf = mysql.run_sql_query(query)\ndata = df.copy() # Keep a copy our original data \n\nquery = query.lower()\n\n# add algorithm data to ml_algorithms\ninsert_data = algo_id\nquery_insert = f\"insert into stats.ml_algorithms (created_at,id,name,type,y_col,sql_used) values(now(),'{algo_id}','{algo_name}','{algo_type}','{y_col}','{query}')\"\ndf2 = mysql.run_sql_insert(query_insert)\n\n\n# basic data stats\nif(show_logs == True):\n    print('df')\n    print(df)\n    print(\"Number of rows in the raw data:\", df.shape[0])\n    print(\"Number of columns in the raw data:\", df.shape[1])\n    print(df.nunique(axis=0))\n    print(df.describe())\n    \n\n\n\n    #print('df.columns')\n    #print(df.columns)\n\n\n# force certain columns to change from object to float\n#cols = cols.index.tolist() \nobject_cols = df.dtypes[df.dtypes == object]  # filtering by string categoricals\nobject_cols = object_cols.index.tolist() \n\n# for column in object_cols:\n#     print('df[column]')\n#     print(df[column])\n#     if df[column].dtype == 'object' and df[column].str.match(r'^-?\\d+\\.?\\d*$').any():\n#         df[column] = df[column].astype(float)\n\n# print(df)\n\ndf['months'] = df['months'].astype(float)\ndf['complete_sales_a'] = df['complete_sales_a'].astype(float)\n\n# columns that are numerical\nnum_cols = df.dtypes[df.dtypes != object]  # filtering by string categoricals\nnum_cols = num_cols.index.tolist() \n#filter y\nnum_cols.remove(y_col)\n\n#\n#NEED TO ADD FILTRATION FOR OUTLIERS HERE\n#\n\n# num_cols = df.select_dtypes('number').columns\nrows_outliers = []\nmin_max_data = {}\n\n\nprint(\"Number of rows before outliers removed: \",df.shape[0])\n\n\nfor col in num_cols:\n    # figure out the stats needed to identify outliers\n    min_v = df[col].min()\n    max_v = df[col].max()\n    mean_v = df[col].mean()\n    stdv_v = df[col].std()\n    threshold_h = mean_v + 3 * stdv_v  # \n    threshold_l = mean_v - 3 * stdv_v  # \n    # Boolean indexing to filter rows\n    # rows_outliers += df[df[col] > threshold_h].index.tolist()\n    # rows_outliers += df[df[col] < threshold_l].index.tolist()\n    # drop outliers\n    df = df.drop(df[df[col] > threshold_h].index)\n    df = df.drop(df[df[col] < threshold_l].index)\n\nprint(\"Number of rows after outliers removed: \",df.shape[0])\n\nfor col in num_cols:\n    # figure out the stats needed to identify outliers\n    min_v_clean = df[col].min()\n    max_v_clean = df[col].max()\n    mean_v_clean = df[col].mean()\n    stdv_v_clean = df[col].std()\n    min_max_data[col] = [min_v_clean, max_v_clean, mean_v_clean, stdv_v_clean]\n# print('min_max_data')\n# print(min_max_data)\n\n\n####\n#sys.exit(0)#shut down code beyond this point\n####\n\n\n# if(show_logs == True):\n#     print('df.dtypes')\n#     print(df.dtypes)\n#     print('num_cols')\n#     print(num_cols)\n#     print('df[months]')\n#     print(df['months'])\n#     print(pd.Categorical(df[num_cols]))\n\n#remove  ids and other unique columns \ndf.dropna(how='all', axis=1, inplace=True)#inplace makes it a copy of itself\n\n#drop columns where every value is unique and the type is object\nall_rows = len(df)\nfor col in df.columns:\n    if len(df[col].unique()) == all_rows :\n        if df[col].dtypes == object:\n            df.drop(col,inplace=True,axis=1)\ndf.nunique(axis=0)\n\n# eliminate rows with nan values before one hot encoding\ndf = df.dropna(axis=0)\n\n#df_y_filtered_list = [x for x in my_list if not isinstance(x, str)]\n# Get a Pd.Series consisting of all the string categoricals\none_hot_encode_cols = df.dtypes[df.dtypes == object]  # filtering by string categoricals\none_hot_encode_cols = one_hot_encode_cols.index.tolist()  # list of categorical fields\n\n# Encode these columns as categoricals so one hot encoding works on split data (if desired)\nfor col in one_hot_encode_cols:\n    df[col] = pd.Categorical(df[col])\n\n# Do the one hot encoding\ndf = pd.get_dummies(df, columns=one_hot_encode_cols)\n#df = pd.get_dummies(data, columns=one_hot_encode_cols)\n#df.info()\n\n# get column headers as predictors - must remove y column first\ndf_x = df.drop(y_col, axis=1)\npredictors = df_x.columns\n\n#split into X and y data for processing\nX = df.drop(y_col, axis=1)\ny = df[y_col]\n\n# fix all nan in X\n# in this case it deletes rows with any nan to clean data \nX = X.dropna(axis=0)\n\n#polynomial transformations\n# pf = PolynomialFeatures(degree=2, include_bias=False,)\n# X = pf.fit_transform(X)\n\n## split data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=72018)\n\n\n## data transformations from standardscaler\nX_train_s = mms.fit_transform(X_train)\nX_test_s = mms.transform(X_test) #using transform instead of fit transform so that it uses the training data only for scaling\njoblib.dump(mms, f'{algo_id}_fit_transform.joblib')\n\n    \n    \n\n","undoManager":{"mark":-2,"position":4,"stack":[[{"start":{"row":0,"column":0},"end":{"row":330,"column":4},"action":"insert","lines":["","import sys","# Surpress warnings:","def warn(*args, **kwargs):","    pass","import warnings","warnings.warn = warn","","#%pylab inline","#get_ipython().run_line_magic('matplotlib', 'inline')","#get_ipython().run_line_magic('config', \"InlineBackend.figure_formats = ['retina']\")","import uuid","import time","import json","import boto3","import pandas as pd","from io import BytesIO","import seaborn as sns","import numpy as np","from scipy import stats","import matplotlib.pyplot as plt","from sklearn.linear_model import Lasso","from sklearn.preprocessing import PolynomialFeatures","from sklearn.model_selection import train_test_split","from sklearn.metrics import r2_score","from sklearn.preprocessing import StandardScaler","from sklearn.preprocessing import MinMaxScaler","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score","import boto3_rds_pandas as mysql","import joblib","#setup logging for algorithm","algo_type = 'Decision Tree'","algo_id =  str(int(time.time())) + '_' + algo_type","algo_name =  'Decision Tree user_risk'","","print('algo_name')","print(algo_name)","","y_col = \"resulted_in_reconcilation\" ## known values of whether or not a transaction resulted in a reconciliation","show_logs = False","","s = StandardScaler()","mms = MinMaxScaler()","sns.set()","","","query = \"SELECT transaction_id, months, transaction_type,  trust_status, complete_sales_a, reconciliations_c,user_has_ssn,resulted_in_reconcilation FROM stats.user_scores_both WHERE 1=1  \"","df = mysql.run_sql_query(query)","data = df.copy() # Keep a copy our original data ","","query = query.lower()","","# add algorithm data to ml_algorithms","insert_data = algo_id","query_insert = f\"insert into stats.ml_algorithms (created_at,id,name,type,y_col,sql_used) values(now(),'{algo_id}','{algo_name}','{algo_type}','{y_col}','{query}')\"","df2 = mysql.run_sql_insert(query_insert)","","","# basic data stats","if(show_logs == True):","    print('df')","    print(df)","    print(\"Number of rows in the raw data:\", df.shape[0])","    print(\"Number of columns in the raw data:\", df.shape[1])","    print(df.nunique(axis=0))","    print(df.describe())","    ","","","","    #print('df.columns')","    #print(df.columns)","","","# force certain columns to change from object to float","#cols = cols.index.tolist() ","object_cols = df.dtypes[df.dtypes == object]  # filtering by string categoricals","object_cols = object_cols.index.tolist() ","","# for column in object_cols:","#     print('df[column]')","#     print(df[column])","#     if df[column].dtype == 'object' and df[column].str.match(r'^-?\\d+\\.?\\d*$').any():","#         df[column] = df[column].astype(float)","","# print(df)","","df['months'] = df['months'].astype(float)","df['complete_sales_a'] = df['complete_sales_a'].astype(float)","","# columns that are numerical","num_cols = df.dtypes[df.dtypes != object]  # filtering by string categoricals","num_cols = num_cols.index.tolist() ","#filter y","num_cols.remove(y_col)","","#","#NEED TO ADD FILTRATION FOR OUTLIERS HERE","#","","# num_cols = df.select_dtypes('number').columns","rows_outliers = []","min_max_data = {}","","","print(\"Number of rows before outliers removed: \",df.shape[0])","","","for col in num_cols:","    # figure out the stats needed to identify outliers","    min_v = df[col].min()","    max_v = df[col].max()","    mean_v = df[col].mean()","    stdv_v = df[col].std()","    threshold_h = mean_v + 3 * stdv_v  # ","    threshold_l = mean_v - 3 * stdv_v  # ","    # Boolean indexing to filter rows","    # rows_outliers += df[df[col] > threshold_h].index.tolist()","    # rows_outliers += df[df[col] < threshold_l].index.tolist()","    # drop outliers","    df = df.drop(df[df[col] > threshold_h].index)","    df = df.drop(df[df[col] < threshold_l].index)","","print(\"Number of rows after outliers removed: \",df.shape[0])","","for col in num_cols:","    # figure out the stats needed to identify outliers","    min_v_clean = df[col].min()","    max_v_clean = df[col].max()","    mean_v_clean = df[col].mean()","    stdv_v_clean = df[col].std()","    min_max_data[col] = [min_v_clean, max_v_clean, mean_v_clean, stdv_v_clean]","# print('min_max_data')","# print(min_max_data)","","","####","#sys.exit(0)#shut down code beyond this point","####","","","# if(show_logs == True):","#     print('df.dtypes')","#     print(df.dtypes)","#     print('num_cols')","#     print(num_cols)","#     print('df[months]')","#     print(df['months'])","#     print(pd.Categorical(df[num_cols]))","","#remove  ids and other unique columns ","df.dropna(how='all', axis=1, inplace=True)#inplace makes it a copy of itself","","#drop columns where every value is unique and the type is object","all_rows = len(df)","for col in df.columns:","    if len(df[col].unique()) == all_rows :","        if df[col].dtypes == object:","            df.drop(col,inplace=True,axis=1)","df.nunique(axis=0)","","# eliminate rows with nan values before one hot encoding","df = df.dropna(axis=0)","","#df_y_filtered_list = [x for x in my_list if not isinstance(x, str)]","# Get a Pd.Series consisting of all the string categoricals","one_hot_encode_cols = df.dtypes[df.dtypes == object]  # filtering by string categoricals","one_hot_encode_cols = one_hot_encode_cols.index.tolist()  # list of categorical fields","","# Encode these columns as categoricals so one hot encoding works on split data (if desired)","for col in one_hot_encode_cols:","    df[col] = pd.Categorical(df[col])","","# Do the one hot encoding","df = pd.get_dummies(df, columns=one_hot_encode_cols)","#df = pd.get_dummies(data, columns=one_hot_encode_cols)","#df.info()","","# get column headers as predictors - must remove y column first","df_x = df.drop(y_col, axis=1)","predictors = df_x.columns","","#split into X and y data for processing","X = df.drop(y_col, axis=1)","y = df[y_col]","","# fix all nan in X","# in this case it deletes rows with any nan to clean data ","X = X.dropna(axis=0)","","#polynomial transformations","# pf = PolynomialFeatures(degree=2, include_bias=False,)","# X = pf.fit_transform(X)","","## split data into training and test sets","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=72018)","","","## data transformations from standardscaler","#from sklearn.preprocessing import StandardScaler","#from sklearn.preprocessing import MinMaxScaler","# X_train_s = s.fit_transform(X_train)","# X_test_s = s.fit_transform(X_test)","X_train_s = mms.fit_transform(X_train)","X_test_s = mms.transform(X_test) #using transform instead of fit transform so that it uses the training data only for scaling","joblib.dump(mms, f'{algo_id}_fit_transform.joblib')","# print(\"Scale:\", mms.scale_)","# print(\"Min:\", mms.min_)","# print(\"Data Min:\", mms.data_min_)","# print(\"Data Max:\", mms.data_max_)","# ###","# sys.exit(0)#shut down code beyond this point","# ###","try:","## LogisticRegression","    from sklearn.linear_model import LogisticRegression","    from sklearn import metrics","    lr = LogisticRegression()","    ","    ","    ","    print('X_train_s',X_train_s)","    print('y_train',y_train)","    import sklearn_models as sklm","    sklm.decision_tree(X_train, X_test, y_train, y_test)","    # lr_result = lr.fit(X_train_s,y_train)","    # pred = lr.predict(X_test)","    ","    ","    ","    joblib.dump(lr, f'{algo_id}.joblib')","#     predictors = df_x.columns","#     #coefficients = lr.coef_","#     #coefficients = 'na'","except Exception as e:","    query_insert4 = f\"update stats.ml_algorithms set parameters = 'failed', object = '{e}', coefficients = 'failed' where id ='{algo_id}' \"","    df4 = mysql.run_sql_insert(query_insert4)","    print(\"An error occurred:\", e)","## sys.exit(0)#shut down code beyond this point","###","","##print(predictors)","##print(coefficients)","#mydict = {k:v for k,v in zip(predictors,lr.coef_[0])}","","#coefficients = lr.coef_","#coefficients = 'na'","#","","","# algorithm_info = {","# \"Coefficients:\" : coefficients,","# \"Intercept:\" :  intercept,","# \"Penalty:\" : penalty,","# \"Solver:\" : solver,","# \"Regularization strength (C):\": C,","# \"Parameters:\": parameters,","# \"Accuracy\" : accuracy,","# \"Precision\" : precision,","# \"Recall\": recall,","# \"F1\" : f1,","# \"roc_auc\" : roc_auc","# }","# print('algorithm_info')","# print(algorithm_info)","# algorithm_info = str(algorithm_info) ","# algorithm_info = \"`\"+ algorithm_info + \"`\"","# print('algorithm_info') ","# print(algorithm_info)","# print('coefficients')","# print(coefficients)","# print('intercept')","# print(intercept)","# print('penalty')","# print(penalty)","# print('solver')","# print(solver)","# print('C')","# print(C)","# print('parameters')","# print(parameters)","# print('accuracy')","# print(accuracy)","","#still an issue with this one ","#print('precision')","#print(precision)","","# print('recall')","# print(recall)","# print('f1')","# print(f1)","# print('roc_auc')","# print(roc_auc)","#parameters = f\"\\\"{parameters}\\\"\"","# # converting python objects to json","# parameters = json.dumps(parameters)","# precision = json.dumps(precision)","# # add algorithm data to ml_algorithms","# #query_insert2 = f\"update stats.ml_algorithms set coefficients = '{coefficients}', intercept = '{intercept}',\tpenalty = '{penalty}',\tsolver = '{solver}',\tC = '{C}',\tparameters = '{parameters}',\taccuracy = '{accuracy}',\tprecision = '{precision}',\trecall = '{recall}',\tf1 = '{f1}',\troc_auc = '{roc_auc}'  where id ='{algo_id}' \"","# query_insert2 = f\"update stats.ml_algorithms set parameters = '{parameters}', object = '{lr}', coefficients = '{coefficients}', intercept = '{intercept}',\tpenalty = '{penalty}',\tsolver = '{solver}',\tC = '{C}',\taccuracy = '{accuracy}', \trecall = '{recall}',\tf1 = '{f1}',\troc_auc = '{roc_auc}'  where id ='{algo_id}' \"","# df3 = mysql.run_sql_insert(query_insert2)","","","# print('mydict')","# print(mydict)","","","# for predictor in predictors:","#     print('predictor')","#     print(predictor)","#     #reset for each predictor","#     v_min = None","#     v_max = None","#     v_avg = None","#     v_stdv = None","    ","#     if(predictor in min_max_data):","#         print(\"min_max_data[predictor]\", predictor)","#         print(min_max_data[predictor])","#         v_min = min_max_data[predictor][0]","#         v_max = min_max_data[predictor][1]","#         v_avg = min_max_data[predictor][2]","#         v_stdv = min_max_data[predictor][3]","#         print(\"v_min\")","#         print(v_min)","        ","#     coeff = mydict[predictor]","#     query_insert4 = f\"insert into stats.ml_coefficients (id,column_name, coefficient, min, max, avg, stdv) values  ('{algo_id}','{predictor}','{coeff}','{v_min}','{v_max}','{v_avg}','{v_stdv}') \"","#     df4 = mysql.run_sql_insert(query_insert4)","    "],"id":1}],[{"start":{"row":239,"column":0},"end":{"row":330,"column":4},"action":"remove","lines":["###","","##print(predictors)","##print(coefficients)","#mydict = {k:v for k,v in zip(predictors,lr.coef_[0])}","","#coefficients = lr.coef_","#coefficients = 'na'","#","","","# algorithm_info = {","# \"Coefficients:\" : coefficients,","# \"Intercept:\" :  intercept,","# \"Penalty:\" : penalty,","# \"Solver:\" : solver,","# \"Regularization strength (C):\": C,","# \"Parameters:\": parameters,","# \"Accuracy\" : accuracy,","# \"Precision\" : precision,","# \"Recall\": recall,","# \"F1\" : f1,","# \"roc_auc\" : roc_auc","# }","# print('algorithm_info')","# print(algorithm_info)","# algorithm_info = str(algorithm_info) ","# algorithm_info = \"`\"+ algorithm_info + \"`\"","# print('algorithm_info') ","# print(algorithm_info)","# print('coefficients')","# print(coefficients)","# print('intercept')","# print(intercept)","# print('penalty')","# print(penalty)","# print('solver')","# print(solver)","# print('C')","# print(C)","# print('parameters')","# print(parameters)","# print('accuracy')","# print(accuracy)","","#still an issue with this one ","#print('precision')","#print(precision)","","# print('recall')","# print(recall)","# print('f1')","# print(f1)","# print('roc_auc')","# print(roc_auc)","#parameters = f\"\\\"{parameters}\\\"\"","# # converting python objects to json","# parameters = json.dumps(parameters)","# precision = json.dumps(precision)","# # add algorithm data to ml_algorithms","# #query_insert2 = f\"update stats.ml_algorithms set coefficients = '{coefficients}', intercept = '{intercept}',\tpenalty = '{penalty}',\tsolver = '{solver}',\tC = '{C}',\tparameters = '{parameters}',\taccuracy = '{accuracy}',\tprecision = '{precision}',\trecall = '{recall}',\tf1 = '{f1}',\troc_auc = '{roc_auc}'  where id ='{algo_id}' \"","# query_insert2 = f\"update stats.ml_algorithms set parameters = '{parameters}', object = '{lr}', coefficients = '{coefficients}', intercept = '{intercept}',\tpenalty = '{penalty}',\tsolver = '{solver}',\tC = '{C}',\taccuracy = '{accuracy}', \trecall = '{recall}',\tf1 = '{f1}',\troc_auc = '{roc_auc}'  where id ='{algo_id}' \"","# df3 = mysql.run_sql_insert(query_insert2)","","","# print('mydict')","# print(mydict)","","","# for predictor in predictors:","#     print('predictor')","#     print(predictor)","#     #reset for each predictor","#     v_min = None","#     v_max = None","#     v_avg = None","#     v_stdv = None","    ","#     if(predictor in min_max_data):","#         print(\"min_max_data[predictor]\", predictor)","#         print(min_max_data[predictor])","#         v_min = min_max_data[predictor][0]","#         v_max = min_max_data[predictor][1]","#         v_avg = min_max_data[predictor][2]","#         v_stdv = min_max_data[predictor][3]","#         print(\"v_min\")","#         print(v_min)","        ","#     coeff = mydict[predictor]","#     query_insert4 = f\"insert into stats.ml_coefficients (id,column_name, coefficient, min, max, avg, stdv) values  ('{algo_id}','{predictor}','{coeff}','{v_min}','{v_max}','{v_avg}','{v_stdv}') \"","#     df4 = mysql.run_sql_insert(query_insert4)","    "],"id":2}],[{"start":{"row":221,"column":0},"end":{"row":238,"column":47},"action":"remove","lines":["    print('X_train_s',X_train_s)","    print('y_train',y_train)","    import sklearn_models as sklm","    sklm.decision_tree(X_train, X_test, y_train, y_test)","    # lr_result = lr.fit(X_train_s,y_train)","    # pred = lr.predict(X_test)","    ","    ","    ","    joblib.dump(lr, f'{algo_id}.joblib')","#     predictors = df_x.columns","#     #coefficients = lr.coef_","#     #coefficients = 'na'","except Exception as e:","    query_insert4 = f\"update stats.ml_algorithms set parameters = 'failed', object = '{e}', coefficients = 'failed' where id ='{algo_id}' \"","    df4 = mysql.run_sql_insert(query_insert4)","    print(\"An error occurred:\", e)","## sys.exit(0)#shut down code beyond this point"],"id":3}],[{"start":{"row":206,"column":0},"end":{"row":218,"column":4},"action":"remove","lines":["# print(\"Scale:\", mms.scale_)","# print(\"Min:\", mms.min_)","# print(\"Data Min:\", mms.data_min_)","# print(\"Data Max:\", mms.data_max_)","# ###","# sys.exit(0)#shut down code beyond this point","# ###","try:","## LogisticRegression","    from sklearn.linear_model import LogisticRegression","    from sklearn import metrics","    lr = LogisticRegression()","    "],"id":4}],[{"start":{"row":199,"column":0},"end":{"row":202,"column":36},"action":"remove","lines":["#from sklearn.preprocessing import StandardScaler","#from sklearn.preprocessing import MinMaxScaler","# X_train_s = s.fit_transform(X_train)","# X_test_s = s.fit_transform(X_test)"],"id":5},{"start":{"row":198,"column":43},"end":{"row":199,"column":0},"action":"remove","lines":["",""]}]]},"ace":{"folds":[],"scrolltop":3024.4298095703125,"scrollleft":0,"selection":{"start":{"row":198,"column":43},"end":{"row":198,"column":43},"isBackwards":false},"options":{"guessTabSize":true,"useWrapMode":false,"wrapToView":true},"firstLineState":{"row":188,"state":"start","mode":"ace/mode/python"}},"timestamp":1714672453774}